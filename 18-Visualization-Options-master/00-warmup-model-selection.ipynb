{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Searching Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.33333333, 0.66666667, 1.        ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0, 1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  2.15443469,  4.64158883, 10.        ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(0, 1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_fit(model, hyperparameters):\n",
    "    pipe(scaling, heres the model)\n",
    "    grid(hyperparameters)\n",
    "    return(whatever you want it to return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "# pipe = make_pipeline(StandardScaler(), )\n",
    "grid = [{'n_neighbors': [2, 4]}]\n",
    "\n",
    "\n",
    "pipe = Pipeline([('classifier', LinearRegression())])\n",
    "\n",
    "# Create space of candidate learning algorithms and their hyperparameters\n",
    "search_space = [{'classifier': [LogisticRegression()],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(0, 4, 10)},\n",
    "                {'classifier': [RandomForestClassifier()],\n",
    "                 'classifier__n_estimators': [10, 100, 1000],\n",
    "                 'classifier__max_features': [1, 2, 3]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipe, search_space, cv=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv': 5,\n",
       " 'error_score': 'raise',\n",
       " 'estimator__memory': None,\n",
       " 'estimator__steps': [('classifier',\n",
       "   LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False))],\n",
       " 'estimator__classifier': LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
       " 'estimator__classifier__copy_X': True,\n",
       " 'estimator__classifier__fit_intercept': True,\n",
       " 'estimator__classifier__n_jobs': 1,\n",
       " 'estimator__classifier__normalize': False,\n",
       " 'estimator': Pipeline(memory=None,\n",
       "      steps=[('classifier', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False))]),\n",
       " 'fit_params': None,\n",
       " 'iid': True,\n",
       " 'n_jobs': 1,\n",
       " 'param_grid': [{'classifier': [LogisticRegression(C=10000.0, class_weight=None, dual=False,\n",
       "              fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "              multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "              solver='liblinear', tol=0.0001, verbose=0, warm_start=False)],\n",
       "   'classifier__penalty': ['l1', 'l2'],\n",
       "   'classifier__C': array([1.00000000e+00, 2.78255940e+00, 7.74263683e+00, 2.15443469e+01,\n",
       "          5.99484250e+01, 1.66810054e+02, 4.64158883e+02, 1.29154967e+03,\n",
       "          3.59381366e+03, 1.00000000e+04])},\n",
       "  {'classifier': [RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                max_depth=None, max_features=1, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "                oob_score=False, random_state=None, verbose=0,\n",
       "                warm_start=False)],\n",
       "   'classifier__n_estimators': [10, 100, 1000],\n",
       "   'classifier__max_features': [1, 2, 3]}],\n",
       " 'pre_dispatch': '2*n_jobs',\n",
       " 'refit': True,\n",
       " 'return_train_score': 'warn',\n",
       " 'scoring': None,\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Using either a manual search or automated example like above, please examine the `ames_housing.csv` file, select appropriate features, perform necessary preprocessing, and compare the results of the following models:\n",
    "\n",
    "- `LinearRegression`\n",
    "- `Ridge`\n",
    "- `Lasso`\n",
    "- `DecisionTreeRegressor`\n",
    "- `RandomForestRegressor`\n",
    "\n",
    "You may want to incorporate a feature selection routine here.  Additionally, try bagging and boosting with two of the models to see if this improves performance. You can also compare the performance of the `xgboost` library.\n",
    "\n",
    "- (10 minutes Preprocessing)\n",
    "- (10 minutes Feature Selection)\n",
    "- (10 minutes Model Implementation)\n",
    "- (5 minutes evaluation and discussion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
