{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Web Scraping\n",
    "\n",
    "To begin, we will examine the reddit page dealing with Machine Learning.  Our goal is to scrape the basic information for posts.\n",
    "\n",
    "![](images/reddit.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e43de851f0b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mRequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbs4\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://www.reddit.com/r/MAchineLearning/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Requests'"
     ]
    }
   ],
   "source": [
    "import Requests\n",
    "import bs4\n",
    "url = 'https://www.reddit.com/r/MAchineLearning/'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3e4673786a70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<!doctype html>\\n<html>\\n  <head>\\n    <title>Too Many Requests</title>\\n    <style>\\n      body {\\n          font: small verdana, arial, helvetica, sans-serif;\\n          width: 600px;\\n          margin: 0 auto;\\n      }\\n\\n      h1 {\\n          height: 40px;\\n          background: transparent url(//www.redditstatic.com/reddit.com.header.png) no-repeat scroll top right;\\n      }\\n    </style>\\n  </head>\\n  <body>\\n    <h1>whoa there, pardner!</h1>\\n    \\n\\n\\n<p>we\\'re sorry, but you appear to be a bot and we\\'ve seen too many requests\\nfrom you lately. we enforce a hard speed limit on requests that appear to come\\nfrom bots to prevent abuse.</p>\\n\\n<p>if you are not a bot but are spoofing one via your browser\\'s user agent\\nstring: please change your user agent string to avoid seeing this message\\nagain.</p>\\n\\n<p>please wait 2 second(s) and try again.</p>\\n\\n    <p>as a reminder to developers, we recommend that clients make no\\n    more than <a href=\"http://github.com/reddit/reddit/wiki/API\">one\\n    request every two seconds</a> to avoid seeing this message.</p>\\n  </body>\\n</html>\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>This is a header</h1>\n",
       "<p class = 'super-paragraph'>This would be a paragraph. <strong>Strong Words</strong> here. </p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h1>This is a header</h1>\n",
    "<p class = 'super-paragraph'>This would be a paragraph. <strong>Strong Words</strong> here. </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_21_Jump_Street_episodes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "# use \"requests\" library to simply get all of the html of a url.  It's stored in a \n",
    "# response object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\\n<head>\\n<meta charset=\"UTF-8\"/>\\n<title>List of 21 Jump Street episodes - Wikipedia</title>\\n<script>document.documentElement.className = document.documentElement.className.replace( /(^|\\\\s)client-nojs(\\\\s|$)/, \"$1client-js$2\" );</script>\\n<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"List_of_21_Jump_Street_episodes\",\"wgTitle\":\"List of 21 Jump Street episodes\",\"wgCurRevisionId\":844038329,\"wgRevisionId\":844038329,\"wgArticleId\":35403829,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Articles needing additional references from May 2012\",\"All articles needing additional references\",\"21 Jump Street\",\"Lists of American crime television series episodes\"],\"wgBreakFrames\":false,\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgSeparatorTransfo'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text[:1000]\n",
    "#text attribute is literally the whole thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#create a beautiful soup object of the html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h2>Contents</h2>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_h2 = soup.find_all('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Season 1 (1987)[edit]\n",
      "Season 2 (1987-88)[edit]\n",
      "Season 3 (1988-89)[edit]\n",
      "Season 4 (1989-90)[edit]\n",
      "Season 5 (1990-91)[edit]\n"
     ]
    }
   ],
   "source": [
    "for header in all_h2[2:7]:\n",
    "    print(header.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table_1 = soup.find('table', {'class': 'wikitable plainrowheaders'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "season_1_titles = table_1.find_all('td', {'class': 'summary'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Pilot\"\n",
      "\"America, What a Town\"\n",
      "\"Don't Pet the Teacher\"\n",
      "\"My Future's So Bright, I Gotta Wear Shades\"\n",
      "\"The Worst Night of Your Life\"\n",
      "\"Gotta Finish the Riff\"\n",
      "\"Bad Influence\"\n",
      "\"Blindsided\"\n",
      "\"Next Generation\"\n",
      "\"Low and Away\"\"Running on Ice\"\n",
      "\"16 Blown to 35\"\n",
      "\"Mean Streets and Pastel Houses\"\n"
     ]
    }
   ],
   "source": [
    "for title in season_1_titles:\n",
    "    print(title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p><i><a href=\"/wiki/21_Jump_Street\" title=\"21 Jump Street\">21 Jump Street</a></i> is an American <a href=\"/wiki/Police_procedural\" title=\"Police procedural\">police procedural</a> <a class=\"mw-redirect\" href=\"/wiki/Crime_drama\" title=\"Crime drama\">crime drama</a> <a class=\"mw-redirect\" href=\"/wiki/Television_series\" title=\"Television series\">television series</a> that aired on the <a href=\"/wiki/Fox_Broadcasting_Company\" title=\"Fox Broadcasting Company\">Fox Network</a> and in first run syndication from April 12, 1987, to April 27, 1991, with a total of 103 <a href=\"/wiki/Episode\" title=\"Episode\">episodes</a>. The series focuses on a squad of youthful-looking undercover police officers investigating crimes in high schools, colleges, and other teenage venues.<sup class=\"reference\" id=\"cite_ref-1\"><a href=\"#cite_note-1\">[1]</a></sup>\n",
       "</p>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href = 'https://www.reddit.com/r/MachineLearning/'> The Reddit Page </a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<a href = 'https://www.reddit.com/r/MachineLearning/'> The Reddit Page </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(soup.find_all('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(soup.find_all('h2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-0e84bc78218e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'data-click-id'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "soup.find('a', {'data-click-id': 'body'})['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "links = []\n",
    "for i in soup.find_all('a', {'data-click-id': 'body'}):\n",
    "    url_link = 'https://www.reddit.com' + i['href']\n",
    "    links.append(url_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "links = []\n",
    "titles = []\n",
    "bodys = []\n",
    "for i in soup.find_all('a', {'data-click-id': 'body'}):\n",
    "    url_link = 'https://www.reddit.com' + i['href']\n",
    "    links.append(url_link)\n",
    "    response = requests.get(url_link)\n",
    "    soup2 = BeautifulSoup(response.text, 'html.parser')\n",
    "    title = soup2.find('h2')\n",
    "    body = soup2.find_all('p')\n",
    "    titles.append(title)\n",
    "    bodys.append(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'links': links, 'title': titles, 'body': bodys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>links</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [links, title, body]\n",
       "Index: []"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia Exercise\n",
    "\n",
    "Scraping Wikipedia tables and adding information found through links.\n",
    "\n",
    "![](images/wiki_table.png)\n",
    "\n",
    "Problem:\n",
    "\n",
    "1. Create a dataframe that contains the information displayed on the Wikipedia page \"List of 2018 Albums\".\n",
    "2. What is Sub Pop releasing in 2018?\n",
    "3. Did Drake put anything out?\n",
    "4. What label is putting out the most music?  Visualize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_2018_albums'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup.find('table', {'class':'wikitable'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key = 'jp8VHqjPsvhrvfamltxmnTGjv'\n",
    "consumer_secret = 'i7GacXRtre4ZQfr3YXt30bkn504FsPWaTU93iu1ObVD3jj4daI'\n",
    "access_token_secret = 'PVfzUaR1wnACkLxJxbFZyAmDs4tIEiFKKEdMElMN0KWP6'\n",
    "access_token = '161038561-Ztk8itAIddAWPPebJhzTpEZQS288BTF5PLnQp5LK'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweepy\n",
    "\n",
    "- Sign into Twitter apps (https://apps.twitter.com/)\n",
    "- Create application and retrieve `consumer_key`, `consumer_secret`, `access_token`, and `access_token_secret`.  \n",
    "- Follow example below filling in your info.  For more info, see the Tweepy documentation [here](http://tweepy.readthedocs.io/en/v3.5.0/getting_started.html#introduction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user = api.get_user('realdonaldtrump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Cindy has voted for our Agenda in the Senate 100% of the time and has my complete and total Endorsement. We need… https://t.co/94PVS8YUFc\n",
      ".@cindyhydesmith has helped me put America First! She’s strong on the Wall, is helping me create Jobs, loves our Ve… https://t.co/9ZwJS8pSBC\n",
      "I have authorized an emergency disaster declaration to provide Hawaii the necessary support ahead of #HurricaneLane… https://t.co/7dWpL4fphZ\n",
      "It was my great honor to host the Foreign Investment Risk Review Modernization Act Roundtable today at the… https://t.co/xkypzKTE17\n",
      "https://t.co/6ZG0P6FRs5\n",
      "https://t.co/6v90Th0zl1\n",
      "https://t.co/3PAVDdfJJr\n",
      "NO COLLUSION - RIGGED WITCH HUNT!\n",
      "I have asked Secretary of State @SecPompeo to closely study the South Africa land and farm seizures and expropriati… https://t.co/iE6t1j1dak\n",
      "The only thing that I have done wrong is to win an election that was expected to be won by Crooked Hillary Clinton… https://t.co/XD0Bh2rq9V\n",
      "I will be interviewed on @foxandfriends by @ainsleyearhardt tomorrow from 6:00 A.M. to 9:00 A.M.  Enjoy!\n",
      "RT @Scavino45: Today, President Trump welcomed the family of @usairforce Tech. Sgt. John A. Chapman, Medal of Honor recipient, to the Oval…\n",
      "https://t.co/15ibBbf34U\n",
      "https://t.co/wYCNmkkaNR\n",
      "Longest bull run in the history of the stock market, congratulations America!\n",
      "https://t.co/mJtO0AFLus\n",
      "https://t.co/OGqKufBeHn\n",
      "Everyone in the path of #HurricaneLane please prepare yourselves, heed the advice of State and local officials, and… https://t.co/fAPukpQIIv\n",
      "Thank you to Democrat Assemblyman Dov Hikind of New York for your very gracious remarks on @foxandfriends for our d… https://t.co/LdLLPAK1i4\n",
      "Michael Cohen plead guilty to two counts of campaign finance violations that are not a crime. President Obama had a… https://t.co/tXtXcWQHHN\n"
     ]
    }
   ],
   "source": [
    "for tweet in user.timeline():\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53957936\n"
     ]
    }
   ],
   "source": [
    "print(user.followers_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for tweet in user.timeline(count = 200):\n",
    "    tweets.append(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['...Cindy has voted for our Agenda in the Senate 100% of the time and has my complete and total Endorsement. We need… https://t.co/94PVS8YUFc',\n",
       " '.@cindyhydesmith has helped me put America First! She’s strong on the Wall, is helping me create Jobs, loves our Ve… https://t.co/9ZwJS8pSBC',\n",
       " 'I have authorized an emergency disaster declaration to provide Hawaii the necessary support ahead of #HurricaneLane… https://t.co/7dWpL4fphZ',\n",
       " 'It was my great honor to host the Foreign Investment Risk Review Modernization Act Roundtable today at the… https://t.co/xkypzKTE17',\n",
       " 'https://t.co/6ZG0P6FRs5']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Table\n",
    "\n",
    "![](images/open_table.png)\n",
    "\n",
    "Finding restaurants in New York City. (https://www.opentable.com/new-york-restaurant-listings)  Is there good Indian food in the Upper West Side?  Where?  What are people saying is good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url2 = 'https://www.yelp.com/search?find_desc=burrito&find_loc=Downtown%2C+Boston%2C+MA+02228&ns=1'\n",
    "response = requests.get(url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soupy = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n  \\n\\n            window.yPageStart = new Date().getTime();\\n\\n            var initialVisibilitySta'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soupy.text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Rests = soupy.find_all('a', {'class': 'biz-name'})\n",
    "titles = []\n",
    "for t in Rests:\n",
    "    titles.append(t.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Zumas Tex Mex Grill',\n",
       " 'Sabroso Taqueria',\n",
       " 'Villa Mexico Cafe',\n",
       " 'Maria’s Taqueria',\n",
       " 'Viva Burrito',\n",
       " 'Anna’s Taqueria',\n",
       " 'Herrera’s',\n",
       " 'Tenoch Mexican',\n",
       " 'Boloco',\n",
       " 'Boloco Atlantic Wharf',\n",
       " 'Cha Cha Cha Taqueria']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
